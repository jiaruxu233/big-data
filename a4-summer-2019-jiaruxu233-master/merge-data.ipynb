{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Merged Dataset\n",
    "\n",
    "In this workbook, you will read in the `trip` and `fare` files. You are welcome to use DataFrame and/or SparkSQL API as you desire as long as it produces the expected results.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Join both datasets such that you get a merged dataset with 21 unique fields. You need to determine how to join the dataset.\n",
    "2. Once you create the merged dataset, you need to convert fields to the following types, since all fields were read is as string:\n",
    "    * pickup_datetime and dropoff_datetime must be TIMESTAMP\n",
    "    * passenger_count and rate_code must be INT\n",
    "    * all other numeric fields must be FLOAT\n",
    "    * the remaining fields stay as STRING\n",
    "3. Save your merged and converted dataset to your own S3 bucket in parquet format.\n",
    "\n",
    "You are welcome to add as many cells as you need below up until the next section. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First start my spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lab-ml\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-29-216.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab-ml</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0a3442f470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = spark.read.parquet(\"s3://bigdatateaching/nyctaxi-2013/parquet/trip/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = spark.read.parquet('s3://bigdatateaching/nyctaxi-2013/parquet/fare/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173179759"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173179759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join those two dataset\n",
    "# to prevent that some columns in trip can not be matched by the corresponding ones in fare\n",
    "import pyspark.sql.functions as F\n",
    "join_df = trip.join(fare, (trip.medallion == fare.medallion) & \n",
    "                          (trip.hack_license == fare.hack_license) & \n",
    "                          (trip.vendor_id == fare.vendor_id) &\n",
    "                          (trip.pickup_datetime == fare.pickup_datetime),'outer').select(\n",
    "                    F.coalesce(trip.medallion,fare.medallion).alias('medallion'),\n",
    "                    F.coalesce(trip.hack_license,fare.hack_license).alias('hack_license'),\n",
    "                    F.coalesce(trip.vendor_id,fare.vendor_id).alias('vendor_id'),\n",
    "                    F.coalesce(trip.pickup_datetime,fare.pickup_datetime).alias('pickup_datetime'),\n",
    "                    trip.rate_code, trip.store_and_fwd_flag, trip.dropoff_datetime, trip.passenger_count,\n",
    "                    trip.trip_time_in_secs, trip.trip_distance, trip.pickup_longitude,\n",
    "                    trip.pickup_latitude, trip.dropoff_longitude, trip.dropoff_latitude,\n",
    "                    fare.payment_type, fare.fare_amount, fare.surcharge, fare.mta_tax,\n",
    "                    fare.tip_amount, fare.tolls_amount, fare.total_amount)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column types as requested\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "timestampcol = ['pickup_datetime','dropoff_datetime']\n",
    "intcol = ['passenger_count','rate_code']\n",
    "floatcol = ['trip_time_in_secs','trip_distance','fare_amount','surcharge',\n",
    "            'mta_tax','tip_amount','tolls_amount','total_amount']\n",
    "\n",
    "for col in timestampcol:\n",
    "    join_df = join_df.withColumn(col, to_timestamp(col))\n",
    "\n",
    "for col in intcol:\n",
    "    join_df = join_df.withColumn(col,join_df[col].cast(IntegerType()))\n",
    "\n",
    "for col in floatcol:\n",
    "    join_df = join_df.withColumn(col,join_df[col].cast('float'))\n",
    "    \n",
    "#join_df = join_df.withColumn(\"pickup_datetime\", to_timestamp('pickup_datetime')) \\\n",
    "#                 .withColumn(\"dropoff_datetime\", to_timestamp('dropoff_datetime')) \\\n",
    "#                 .withColumn('passenger_count', join_df['passenger_count'].cast(IntegerType())) \\\n",
    "#                 .withColumn('rate_code', join_df['rate_code'].cast(IntegerType())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|00005007A9F30E289...|02015F5B7D1884620...|      CMT|2013-05-20 02:15:05|        1|                 N|2013-05-20 02:28:51|              2|            827.0|          5.3|        -74.0084|      40.740517|       -73.965195|       40.801292|         CRD|       17.5|      0.5|    0.5|       3.7|         0.0|        22.2|\n",
      "|00005007A9F30E289...|0649DA10C83DE7C6A...|      CMT|2013-05-14 20:28:38|        1|                 N|2013-05-14 20:37:35|              1|            536.0|          1.7|      -73.979301|       40.73579|        -74.00354|       40.740307|         CRD|        8.5|      0.5|    0.5|       1.9|         0.0|        11.4|\n",
      "|00005007A9F30E289...|0FD461760B482C0B2...|      CMT|2013-11-18 01:41:18|        1|                 N|2013-11-18 01:53:22|              1|            723.0|          3.2|      -73.987175|      40.722256|       -73.949669|       40.715641|         CSH|       12.0|      0.5|    0.5|       0.0|         0.0|        13.0|\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 19:17:33|        1|                 N|2013-07-30 19:25:54|              1|            501.0|          1.1|      -73.980736|      40.753197|       -73.971275|       40.764122|         CRD|        7.0|      1.0|    0.5|       1.5|         0.0|        10.0|\n",
      "|00005007A9F30E289...|18F924B3A1BC019F9...|      CMT|2013-07-10 01:09:09|        1|                 Y|2013-07-10 01:13:50|              2|            281.0|          0.8|      -74.006104|      40.734661|        -74.00618|       40.743752|         CRD|        5.0|      0.5|    0.5|       1.0|         0.0|         7.0|\n",
      "|00005007A9F30E289...|1C533BFDA3D6D892F...|      CMT|2013-05-07 15:23:15|        1|                 N|2013-05-07 15:37:46|              1|            870.0|          1.5|      -73.986496|       40.73341|       -73.984604|       40.750008|         CRD|       10.5|      0.0|    0.5|      2.75|         0.0|       13.75|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-05 11:40:40|        1|                 N|2013-10-05 11:57:20|              3|           1000.0|          3.7|      -73.966492|      40.789413|       -74.003059|       40.745758|         CRD|       15.0|      0.0|    0.5|      3.87|         0.0|       19.37|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|        1|                 Y|2013-10-11 11:23:11|              1|            285.0|          0.9|      -73.997002|       40.72245|       -73.986115|       40.729259|         CRD|        5.5|      0.0|    0.5|       1.0|         0.0|         7.0|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-14 14:41:18|        1|                 N|2013-10-14 15:02:17|              1|           1259.0|          1.5|      -73.960266|      40.781761|       -73.967461|       40.763233|         CSH|       13.0|      0.0|    0.5|       0.0|         0.0|        13.5|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-18 12:06:37|        1|                 N|2013-10-18 12:13:09|              2|            392.0|          1.1|      -73.962921|      40.772476|       -73.954216|       40.786285|         CRD|        6.5|      0.0|    0.5|       1.0|         0.0|         8.0|\n",
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cells, please provide the requested code and output. Do not change the order and/or structure of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, provide the code that saves your merged dataset to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.write.parquet('s3a://jiaruxu-bigdatasummer/a4/merged_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the schema of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the first 10 records of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|00005007A9F30E289...|02015F5B7D1884620...|      CMT|2013-05-20 02:15:05|        1|                 N|2013-05-20 02:28:51|              2|            827.0|          5.3|        -74.0084|      40.740517|       -73.965195|       40.801292|         CRD|       17.5|      0.5|    0.5|       3.7|         0.0|        22.2|\n",
      "|00005007A9F30E289...|0649DA10C83DE7C6A...|      CMT|2013-05-14 20:28:38|        1|                 N|2013-05-14 20:37:35|              1|            536.0|          1.7|      -73.979301|       40.73579|        -74.00354|       40.740307|         CRD|        8.5|      0.5|    0.5|       1.9|         0.0|        11.4|\n",
      "|00005007A9F30E289...|0FD461760B482C0B2...|      CMT|2013-11-18 01:41:18|        1|                 N|2013-11-18 01:53:22|              1|            723.0|          3.2|      -73.987175|      40.722256|       -73.949669|       40.715641|         CSH|       12.0|      0.5|    0.5|       0.0|         0.0|        13.0|\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 19:17:33|        1|                 N|2013-07-30 19:25:54|              1|            501.0|          1.1|      -73.980736|      40.753197|       -73.971275|       40.764122|         CRD|        7.0|      1.0|    0.5|       1.5|         0.0|        10.0|\n",
      "|00005007A9F30E289...|18F924B3A1BC019F9...|      CMT|2013-07-10 01:09:09|        1|                 Y|2013-07-10 01:13:50|              2|            281.0|          0.8|      -74.006104|      40.734661|        -74.00618|       40.743752|         CRD|        5.0|      0.5|    0.5|       1.0|         0.0|         7.0|\n",
      "|00005007A9F30E289...|1C533BFDA3D6D892F...|      CMT|2013-05-07 15:23:15|        1|                 N|2013-05-07 15:37:46|              1|            870.0|          1.5|      -73.986496|       40.73341|       -73.984604|       40.750008|         CRD|       10.5|      0.0|    0.5|      2.75|         0.0|       13.75|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-05 11:40:40|        1|                 N|2013-10-05 11:57:20|              3|           1000.0|          3.7|      -73.966492|      40.789413|       -74.003059|       40.745758|         CRD|       15.0|      0.0|    0.5|      3.87|         0.0|       19.37|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|        1|                 Y|2013-10-11 11:23:11|              1|            285.0|          0.9|      -73.997002|       40.72245|       -73.986115|       40.729259|         CRD|        5.5|      0.0|    0.5|       1.0|         0.0|         7.0|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-14 14:41:18|        1|                 N|2013-10-14 15:02:17|              1|           1259.0|          1.5|      -73.960266|      40.781761|       -73.967461|       40.763233|         CSH|       13.0|      0.0|    0.5|       0.0|         0.0|        13.5|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-18 12:06:37|        1|                 N|2013-10-18 12:13:09|              2|            392.0|          1.1|      -73.962921|      40.772476|       -73.954216|       40.786285|         CRD|        6.5|      0.0|    0.5|       1.0|         0.0|         8.0|\n",
      "+--------------------+--------------------+---------+-------------------+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the row count of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173185091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
