{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Predictive Model(s)\n",
    "\n",
    "In this workbook, you will read the merged dataset you created previously and you will create transformer, estimators and pipelines to build a binary classification model to predict wether a trip has a tip or not.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Read in your merged dataset\n",
    "2. Use transformes and encoders to perform feature engineering\n",
    "3. Split into training and testing\n",
    "4. Build `LogisticRegression` model(s) and train them using pipelines\n",
    "5. Evaluate the performance of the model(s) using `BinaryClassificationMetrics`\n",
    "\n",
    "You are welcome to add as many cells as you need below up until the next section. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lab-ml\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-25-17.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab-ml</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f63c82b7438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = spark.read.parquet(\"s3://jiaruxu-bigdatasummer/a4/merged_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tip amount to double type to be able to apply binarizer\n",
    "join_df = join_df.withColumn('tip_amount',join_df['tip_amount'].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler,Binarizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our target to a binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_tip = Binarizer(threshold=0.0, inputCol=\"tip_amount\", outputCol=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = bi_tip.transform(join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform timestamp features into year,month and dayofweek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pickup datetime\n",
    "join_df = join_df.withColumn('pickup_year', year('pickup_datetime'))\n",
    "join_df = join_df.withColumn('pickup_month', month('pickup_datetime'))\n",
    "join_df = join_df.withColumn('pickup_dayofweek', dayofweek('pickup_datetime'))\n",
    "\n",
    "#for dropoff datetime\n",
    "join_df = join_df.withColumn('dropoff_year', year('dropoff_datetime'))\n",
    "join_df = join_df.withColumn('dropoff_month', month('dropoff_datetime'))\n",
    "join_df = join_df.withColumn('dropoff_dayofweek', dayofweek('dropoff_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = join_df.filter(\"medallion != ''\" ).filter(\"hack_license != ''\" ) \\\n",
    "                 .filter(\"vendor_id != ''\" ).filter(\"store_and_fwd_flag != ''\" ) \\\n",
    "                 .filter(\"pickup_longitude != ''\" ).filter(\"pickup_latitude != ''\" ) \\\n",
    "                 .filter(\"dropoff_longitude != ''\" ).filter(\"dropoff_latitude != ''\" ) \\\n",
    "                 .filter(\"payment_type != ''\" ).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = false)\n",
      " |-- trip_distance: float (nullable = false)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = false)\n",
      " |-- surcharge: float (nullable = false)\n",
      " |-- mta_tax: float (nullable = false)\n",
      " |-- tip_amount: double (nullable = false)\n",
      " |-- tolls_amount: float (nullable = false)\n",
      " |-- total_amount: float (nullable = false)\n",
      " |-- labels: double (nullable = false)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_dayofweek: integer (nullable = true)\n",
      " |-- dropoff_year: integer (nullable = true)\n",
      " |-- dropoff_month: integer (nullable = true)\n",
      " |-- dropoff_dayofweek: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 69327594\n",
      "Number of testing records : 15601460\n",
      "Number of prediction records : 1731860\n"
     ]
    }
   ],
   "source": [
    "splitted_data = join_df.randomSplit([0.8, 0.18, 0.02], 666)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n",
    "print(\"Number of prediction records : \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestampcol = ['pickup_datetime','dropoff_datetime']\n",
    "#                 ---- >\n",
    "#               ['pickup_year','pickup_month','pickup_dayofweek',\n",
    "#                'dropoff_year', 'dropoff_month', 'dropoff_dayofweek']\n",
    "\n",
    "# intcol = ['passenger_count','rate_code']\n",
    "\n",
    "# floatcol = ['trip_time_in_secs','trip_distance','fare_amount','surcharge',\n",
    "#            'mta_tax','tip_amount','tolls_amount','total_amount']\n",
    "\n",
    "# strcol = ['medallion','hack_license','vendor_id','store_and_fwd_flag','pickup_longitude',\n",
    "#           'pickup_latitude','dropoff_longitude','dropoff_latitude','payment_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stringIndexer\n",
    "stringIndexer_medallion = StringIndexer(inputCol=\"medallion\", outputCol=\"medallion_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_hack_license = StringIndexer(inputCol=\"hack_license\", outputCol=\"hack_license_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_vendor_id = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendor_id_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_store_and_fwd_flag = StringIndexer(inputCol=\"store_and_fwd_flag\", outputCol=\"store_and_fwd_flag_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_pickup_longitude = StringIndexer(inputCol=\"pickup_longitude\", outputCol=\"pickup_longitude_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_pickup_latitude = StringIndexer(inputCol=\"pickup_latitude\", outputCol=\"pickup_latitude_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_dropoff_longitude = StringIndexer(inputCol=\"dropoff_longitude\", outputCol=\"dropoff_longitude_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_dropoff_latitude = StringIndexer(inputCol=\"dropoff_latitude\", outputCol=\"dropoff_latitude_IX\").setHandleInvalid(\"keep\")\n",
    "stringIndexer_payment_type = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_IX\").setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(\n",
    "    inputCols=[\"medallion_IX\", \"hack_license_IX\", \"vendor_id_IX\", \"store_and_fwd_flag_IX\",\n",
    "              \"pickup_longitude_IX\",\"pickup_latitude_IX\",\"dropoff_longitude_IX\",\"dropoff_latitude_IX\",\n",
    "              \"payment_type_IX\", # 9 stringindexder columns\n",
    "              \"trip_time_in_secs\",\"trip_distance\", \"fare_amount\", \"surcharge\" , \"mta_tax\",\n",
    "              \"tolls_amount\", \"total_amount\", # 7 float columns\n",
    "               \"passenger_count\",\"rate_code\", # 2 integer columns\n",
    "               \"pickup_year\", \"pickup_month\",\"pickup_dayofweek\",\n",
    "               \"dropoff_year\",\"dropoff_month\",\"dropoff_dayofweek\"], #  6 timestamp transformed columns\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_82892944eb1d"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_df.select('tip_amount').filter((join_df.tip_amount == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logit = LogisticRegression(labelCol=\"labels\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logit = Pipeline(stages=[stringIndexer_medallion, stringIndexer_hack_license, \n",
    "                               stringIndexer_vendor_id, stringIndexer_store_and_fwd_flag,\n",
    "                               stringIndexer_pickup_longitude, stringIndexer_pickup_latitude,\n",
    "                               stringIndexer_payment_type, stringIndexer_dropoff_longitude,\n",
    "                               stringIndexer_dropoff_latitude,\n",
    "                               vectorAssembler_features, \n",
    "                               logit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit = pipeline_logit.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_logit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to s3 bucket in case of exceeding memory limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write.parquet('s3a://jiaruxu-bigdatasummer/a4/predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### execute when restart the kernel #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from s3 bucket \n",
    "predictions = spark.read.parquet(\"s3://jiaruxu-bigdatasummer/a4/predictions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select('prediction','labels').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "evaluatorLogit = BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cells, please provide the requested code and output. Do not change the order and/or structure of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, print the Area Under the Curve (AUC) for your binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9869663209409401"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorLogit.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, provide the code that saves your model your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit.save('s3a://jiaruxu-bigdatasummer/a4/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
